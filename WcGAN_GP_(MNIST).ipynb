{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WcGAN-GP (MNIST).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hHr6fnk6hyi3",
        "EPIm-RHcCdo5",
        "kghQIj3FChRc",
        "dGDnrZ8GExVt",
        "xA9WbvFpF64H",
        "v6RCaCLGF-Px",
        "zH07XncpGDsS",
        "YeM9gJ08HfrM",
        "RiHEQRTRHlvf",
        "fDHWOUfLI9Ym"
      ],
      "authorship_tag": "ABX9TyMLyV2ewMRTm6klnkTAKf3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RGivisiez/WcGAN-GP-MNIST/blob/main/WcGAN_GP_(MNIST).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "\n",
        "In this notebook, I will use a conditional GAN to generate numbers of the MNIST dataset. Because GANs requires a lot of computational resources, I will restrict the dataset size and the number of classes used. My main objective is to see the process of training a GAN.\n",
        "\n",
        "To keep track of GAN's discriminator loss and visualize the quality of images generated, I will use the TensorBoard.\n",
        "\n",
        "> To prevent the well know mode colapse that a vannila GAN presents, I will use the Wasserstein loss with a gradient penalty (WGAN-GP).\n",
        "\n",
        "> **Tips:** Typically, the discriminator (critic) loss is expected to have negative values and fall very quickly at the beginning of the training, then rise again to near 0. This behaviour is expected of a stable GAN.\n",
        "\n",
        "![epoch_d_loss](img/epoch_d_loss.svg)"
      ],
      "metadata": {
        "id": "1LeWt1-0h2kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "hHr6fnk6hyi3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeJy4ohuaN26",
        "outputId": "961c51b9-6364-4bcb-95f3-168ba6719dab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "0DJ2cp_8aeFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the dataset"
      ],
      "metadata": {
        "id": "EPIm-RHcCdo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downlad and normalize imagens\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "norm_image = 255 / 2.0\n",
        "\n",
        "train_images = (train_images - norm_image) / norm_image  # Normalize the images to [-1, 1]"
      ],
      "metadata": {
        "id": "2BVluXbyared",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set the train size and select the classes to consider (Default is 0 and 2).\n",
        "\n",
        "train_size = 512 #@param {type:\"integer\"}\n",
        "class_to_consider = [0, 2] #@param {type:\"raw\"}\n",
        "num_class = len(class_to_consider)\n",
        "\n",
        "print('Train size: {0} \\nClasses: {1}'.format(train_size, class_to_consider))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "uFL_YqdR_LDO",
        "outputId": "1302a2f3-841e-4365-e329-0ff4a8e247a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 512 \n",
            "Classes: [0, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restrict the train size and select the classes labels\n",
        "\n",
        "mask = train_labels == class_to_consider[0]\n",
        "\n",
        "for c in class_to_consider[1:]:\n",
        "  mask = mask + (train_labels == c)\n",
        "\n",
        "train_images = train_images[mask]\n",
        "train_labels = train_labels[mask]\n",
        "\n",
        "train_images = train_images[:train_size]\n",
        "train_labels = train_labels[:train_size]"
      ],
      "metadata": {
        "id": "xSpsvTAQjtm7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buffer size must be big. \n",
        "# A rule of thumbs is to choose it equal\n",
        "# to the dataset size.\n",
        "\n",
        "BUFFER_SIZE = train_images.shape[0]\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "SemxzkS1bOro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make the dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "WBAgb3uvbiVO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "kghQIj3FChRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_dim = 128"
      ],
      "metadata": {
        "id": "jB8Tu32CzIGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_input_shape = (noise_dim + num_class, )\n",
        "disc_input_shape = (train_images.shape[1],\n",
        "                    train_images.shape[2],\n",
        "                    1 + num_class,)"
      ],
      "metadata": {
        "id": "jkfGbg0Cwb7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator1():\n",
        "\n",
        "  model = tf.keras.Sequential(name='generator1')\n",
        "\n",
        "  model.add(layers.Dense(7*7*256, input_shape=gen_input_shape))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Reshape([7, 7, 256]))\n",
        "  model.add(layers.Conv2DTranspose(128, 5, 1, 'same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(64, 5, 2, 'same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(1, 5, 2, 'same', activation='tanh'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "qP2n6BudbsWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator1():\n",
        "\n",
        "  model = tf.keras.Sequential(name='discriminator1')\n",
        "\n",
        "  model.add(layers.Conv2D(64, 5, 2, 'same', input_shape=disc_input_shape))\n",
        "  model.add(layers.LeakyReLU())\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Conv2D(128, 5, 2, 'same'))\n",
        "  model.add(layers.LeakyReLU())\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  \n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(1))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "xLkWmyqDhsbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample_block(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    up_size=(2, 2),\n",
        "    padding=\"same\",\n",
        "    use_bn=False,\n",
        "    use_bias=True,\n",
        "    use_dropout=False,\n",
        "    drop_value=0.3,\n",
        "):\n",
        "    x = layers.UpSampling2D(up_size)(x)\n",
        "    x = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
        "    )(x)\n",
        "\n",
        "    if use_bn:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    if use_dropout:\n",
        "        x = layers.Dropout(drop_value)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def make_generator2():\n",
        "    noise = layers.Input(shape=gen_input_shape)\n",
        "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Reshape((4, 4, 256))(x)\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        128,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        64,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
        "    )\n",
        "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
        "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
        "    x = layers.Cropping2D((2, 2))(x)\n",
        "    \n",
        "    g_model = keras.models.Model(noise, x, name=\"generator2\")\n",
        "    return g_model"
      ],
      "metadata": {
        "id": "6xtzj9FuWLMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"same\",\n",
        "    use_bias=True,\n",
        "    use_bn=False,\n",
        "    use_dropout=False,\n",
        "    drop_value=0.5,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
        "    )(x)\n",
        "    if use_bn:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = activation(x)\n",
        "    if use_dropout:\n",
        "        x = layers.Dropout(drop_value)(x)\n",
        "    return x\n",
        "\n",
        "def make_discriminator2():\n",
        "    img_input = layers.Input(shape=disc_input_shape)\n",
        "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
        "    x = layers.ZeroPadding2D((2, 2))(img_input)\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        64,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        use_bias=True,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_dropout=False,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        128,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=True,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        256,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=True,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        512,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=False,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1)(x)\n",
        "    \n",
        "    d_model = keras.models.Model(img_input, x, name=\"discriminator2\")\n",
        "    return d_model"
      ],
      "metadata": {
        "id": "ciYOhNdWVo-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WcGAN-GP class"
      ],
      "metadata": {
        "id": "dGDnrZ8GExVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WGAN(Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim,\n",
        "                 d_extra_steps=3, gp_weight=10.0,):\n",
        "        super(WGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_extra_steps = d_extra_steps\n",
        "        self.gp_weight = gp_weight\n",
        "\n",
        "    def compile(self, disc_opt, gen_opt, d_loss_fn, g_loss_fn):\n",
        "        super(WGAN, self).compile()\n",
        "        self.disc_opt = disc_opt\n",
        "        self.gen_opt = gen_opt\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "            This loss is calculated on an interpolated image\n",
        "            and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        # Get the interpolated image\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1])\n",
        "        interpolated = real_images * alpha + fake_images * (1 - alpha)\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # 1. Get the discriminator output for this interpolated image.\n",
        "            pred = self.discriminator(interpolated, training=True)\n",
        "\n",
        "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        \n",
        "        # 3. Calculate the norm of the gradients.\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        \n",
        "        return gp\n",
        "\n",
        "    # Override the training step function of the Model class\n",
        "    def train_step(self, data):\n",
        "\n",
        "      images, labels = data\n",
        "\n",
        "      batch_size = tf.shape(images)[0]\n",
        "\n",
        "      one_hot_labels = tf.one_hot(labels, depth=num_class)\n",
        "\n",
        "      image_one_hot_labels = tf.repeat(one_hot_labels,\n",
        "                                 repeats=[disc_input_shape[0] * disc_input_shape[1]])\n",
        "\n",
        "      image_one_hot_labels = tf.reshape(image_one_hot_labels,\n",
        "                                        (-1, disc_input_shape[0], disc_input_shape[1], num_class))\n",
        "\n",
        "      #---------Discriminator loop--------------\n",
        "\n",
        "      for i in range(self.d_extra_steps):\n",
        "\n",
        "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "\n",
        "        noise_labels = tf.concat([noise, one_hot_labels], axis=1)\n",
        "\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "          \n",
        "          G_output = self.generator(noise_labels, training=True)\n",
        "          G_output = tf.concat([G_output, image_one_hot_labels], -1)\n",
        "          images_and_labels = tf.concat([images, image_one_hot_labels], -1)\n",
        "  \n",
        "          D_fake_pred = self.discriminator(G_output, training=True)\n",
        "\n",
        "          D_real_pred = self.discriminator(images_and_labels, training=True)\n",
        "\n",
        "          gp = self.gradient_penalty(batch_size, images_and_labels, G_output)\n",
        "          D_loss = self.d_loss_fn(D_real_pred, D_fake_pred, self.gp_weight, gp)\n",
        "\n",
        "        D_grad = disc_tape.gradient(D_loss, self.discriminator.trainable_variables)\n",
        "        disc_opt.apply_gradients(zip(D_grad, self.discriminator.trainable_variables))\n",
        "      \n",
        "      #------Generator loop-------------\n",
        "\n",
        "      noise = tf.random.normal([batch_size, self.latent_dim])\n",
        "\n",
        "      noise_labels = tf.concat([noise, one_hot_labels], axis=1)\n",
        "\n",
        "      with tf.GradientTape() as gen_tape:\n",
        "\n",
        "        G_output = self.generator(noise_labels, training=True)\n",
        "        G_output = tf.concat([G_output, image_one_hot_labels], -1)\n",
        "\n",
        "        D_fake_pred = self.discriminator(G_output, training=True)\n",
        "\n",
        "        G_loss = self.g_loss_fn(D_fake_pred)\n",
        "\n",
        "      G_grad = gen_tape.gradient(G_loss, self.generator.trainable_variables)\n",
        "      gen_opt.apply_gradients(zip(G_grad, self.generator.trainable_variables))\n",
        "\n",
        "      return {'d_loss': D_loss, \"g_loss\": G_loss}"
      ],
      "metadata": {
        "id": "p5yjSb7XsVm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks"
      ],
      "metadata": {
        "id": "xA9WbvFpF64H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path to save the callbacks\n",
        "logdir = \"logs/Model-2\"#os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "\n",
        "image_writer = tf.summary.create_file_writer(logdir + '/img')\n",
        "\n",
        "print('Path logs: {0}'.format(logdir))"
      ],
      "metadata": {
        "id": "j4gev3GopsAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c993ce-1114-48a8-932c-3d5ad0619b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path logs: logs/Model-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,\n",
        "                                                      histogram_freq=5,\n",
        "                                                      profile_batch = '2,3')"
      ],
      "metadata": {
        "id": "TrrAT4p3LD1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Help functions"
      ],
      "metadata": {
        "id": "v6RCaCLGF-Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_to_image(figure):\n",
        "\n",
        "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "  \n",
        "  # Save the plot to a PNG in memory.\n",
        "  buf = io.BytesIO()\n",
        "  plt.savefig(buf, format='png')\n",
        "\n",
        "  # Closing the figure prevents it from being displayed directly inside\n",
        "  # the notebook.\n",
        "  plt.close(figure)\n",
        "  buf.seek(0)\n",
        "\n",
        "  # Convert PNG buffer to TF image\n",
        "  image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "  \n",
        "  # Add the batch dimension\n",
        "  image = tf.expand_dims(image, 0)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "nnmBlcC_qc8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GeneratorImages**: callback to save images for TensorBoard\n",
        "\n",
        "> **Warning**: This function use matplotlib and will slowdown the code, consider exclude GeneratorImages from the callback calls in model.fit()."
      ],
      "metadata": {
        "id": "zH07XncpGDsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorImages(keras.callbacks.Callback):\n",
        "  def __init__(self, fixed_seed, save_step=10):\n",
        "    self.fixed_seed = fixed_seed\n",
        "    self.save_step = save_step\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "    if epoch % self.save_step == 0:\n",
        "\n",
        "      display.clear_output(wait=True)\n",
        "      print('Epoch {0}'.format(epoch + 1))\n",
        "\n",
        "      labels = tf.range(0, num_class)\n",
        "      one_hot_labels = tf.one_hot(labels, num_class)\n",
        "      noise_labels = tf.concat([self.fixed_seed, one_hot_labels], axis=1)\n",
        "      predictions = self.model.generator(noise_labels, training=False)\n",
        "\n",
        "      predictions_img = predictions[:, :, :, 0] * 127.5 + 127.5\n",
        "      \n",
        "      fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "      for i in range(predictions_img.shape[0]):\n",
        "        plt.imshow(predictions_img[i, :, :], cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "        with image_writer.as_default():\n",
        "          tf.summary.image(\"Generator Images - {}\".format(i), plot_to_image(fig), step=epoch)"
      ],
      "metadata": {
        "id": "lxO-WW1910Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss function"
      ],
      "metadata": {
        "id": "YeM9gJ08HfrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def W_gen_loss(fake_pred):\n",
        "  return -tf.reduce_mean(fake_pred)\n",
        "\n",
        "def W_disc_loss(real_pred, fake_pred, eps, grad_pen):\n",
        "  return -tf.reduce_mean(real_pred) + tf.reduce_mean(fake_pred) + eps * grad_pen"
      ],
      "metadata": {
        "id": "fhbYOIyS_cz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model build and compile\n",
        "\n",
        "There are two models that can be used:\n",
        "\n",
        "  1. `make_generator1()` or `make_discriminator1()`\n",
        "  2. `make_generator2()` or `make_discriminator2()`\n",
        "\n",
        "> By default `make_discriminator2()` and `make_generator2()` are used, because they give better results."
      ],
      "metadata": {
        "id": "RiHEQRTRHlvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Generator = make_generator2()\n",
        "Discriminator = make_discriminator2()"
      ],
      "metadata": {
        "id": "DeBung0fy7I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc_opt = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
        "gen_opt = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)"
      ],
      "metadata": {
        "id": "yNdX8bMTRBRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = WGAN(Discriminator, Generator, noise_dim, d_extra_steps=3)"
      ],
      "metadata": {
        "id": "em1OC_WTsVkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(disc_opt, gen_opt, W_disc_loss, W_gen_loss)"
      ],
      "metadata": {
        "id": "fcKSMU0XsVij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model train"
      ],
      "metadata": {
        "id": "fDHWOUfLI9Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "EPOCH =  700#@param {type:\"integer\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "auBcxksOJmWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run TensorBoard\n",
        "%tensorboard --logdir logs --host 0.0.0.0"
      ],
      "metadata": {
        "id": "zUQa-7VDQKV6",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "outputId": "aa3b2c69-f743-46fb-d78e-cc9fea68c099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-40ea7b25e560a2f4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-40ea7b25e560a2f4\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed used to creat new images at every epoch using the Generator.\n",
        "# With a fixed seed, it is possible to see how the generator improves\n",
        "# at every epoch.\n",
        "fixed_seed = tf.random.normal([num_class, noise_dim], seed=42)"
      ],
      "metadata": {
        "id": "-6d16pXHmSth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(train_dataset,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 epochs=EPOCH,\n",
        "                 callbacks=[GeneratorImages(fixed_seed),\n",
        "                            tensorboard_callback,\n",
        "                            ]\n",
        "                 )"
      ],
      "metadata": {
        "id": "LZISf4b8sVgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74d86a0-e8b4-4537-80a2-ab1e4077494e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 691\n",
            "4/4 [==============================] - 5s 1s/step - d_loss: -3.4008 - g_loss: -9.3778\n",
            "Epoch 692/700\n",
            "4/4 [==============================] - 3s 831ms/step - d_loss: -3.2248 - g_loss: -9.0447\n",
            "Epoch 693/700\n",
            "4/4 [==============================] - 3s 793ms/step - d_loss: -3.6159 - g_loss: -9.1009\n",
            "Epoch 694/700\n",
            "4/4 [==============================] - 3s 790ms/step - d_loss: -3.4703 - g_loss: -8.7952\n",
            "Epoch 695/700\n",
            "4/4 [==============================] - 3s 791ms/step - d_loss: -3.3512 - g_loss: -9.5917\n",
            "Epoch 696/700\n",
            "4/4 [==============================] - 4s 1s/step - d_loss: -3.1629 - g_loss: -9.0833\n",
            "Epoch 697/700\n",
            "4/4 [==============================] - 3s 781ms/step - d_loss: -2.8746 - g_loss: -8.8627\n",
            "Epoch 698/700\n",
            "4/4 [==============================] - 3s 780ms/step - d_loss: -3.3518 - g_loss: -8.4206\n",
            "Epoch 699/700\n",
            "4/4 [==============================] - 3s 791ms/step - d_loss: -3.1163 - g_loss: -8.6230\n",
            "Epoch 700/700\n",
            "4/4 [==============================] - 3s 790ms/step - d_loss: -2.9158 - g_loss: -6.5741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate new images using the Discriminator"
      ],
      "metadata": {
        "id": "2lHcdW-tPXBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number o images\n",
        "n_images = 4 #@param {type:\"integer\"}\n",
        "\n",
        "plt.close()\n",
        "\n",
        "for _ in range(n_images):\n",
        "  \n",
        "  fixed_seed = tf.random.normal([num_class, noise_dim])\n",
        "\n",
        "  labels = tf.range(0, num_class)\n",
        "  one_hot_labels = tf.one_hot(labels, num_class)\n",
        "  noise_labels = tf.concat([fixed_seed, one_hot_labels], axis=1)\n",
        "  predictions = Generator(noise_labels, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(num_class+1, 2))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(1, num_class, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "C1G-eV4nCvHk",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "57b8cac6-1659-42db-81c3-26a578582949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x200 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAB9CAYAAACGRw4aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAO5UlEQVR4nO2dT2xWRRfGp/4BrIihUIFiITRCMBgxwU1DSIgUC0T+JJRYEwghgZi4adKEhZDUBKN1UY0REBYEUtQqSNNFF4ACGmCDCYJhIwYwASOKEFFbrYrw7e53ztO359x5+7Zo5vmt7sm8vXfunTu988w5c6bszp07dwIhJAnuudsVIIQMH+zwhCQEOzwhCcEOT0hCsMMTkhDs8IQkBDs8IQnBDk9IQtyX+4f32T8tKyvLfdHbt28PWHbvvfea58U4IbTvuef//8P++ecfsx5ezJG8tjxvCP3vAc8l78P7WyTm93jdv//+2zx3MWCbYP3kc8K6YvtZ9xbz20K/l+UxbeuB5/L+VpZ776+Hda1bt24p23vfQ+AXnpCkYIcnJCHY4QlJiNwa3tNxEtQpqL1izuWd29JIniZErHqOGjVKlf3111/KzqOf8tYLyy2N7M0HlAKcv8H2s575yJEjzXNbcw7WvIhXT0/ve9e6//77s+PYto6Z+4mZS0C851EIfuEJSQh2eEISgh2ekITIreE9pAbyfNSoa2S5p9FjtVhMPay5BdRtaMf4ar17snTeYDRfsXjtKeuEZah/Efl7bA9vzsX6PbYP+qwRq028ORfrecT47AvZVt8oBn7hCUkIdnhCEiL3kH4wQ1ZvCGS5dWLrId0peN2+vj5le8M8eS0c7nuuGWs46g3hEcv98m9ISTiYMFUJPjN06aELzwotxd/GhrjGuFmx3tI96L1jSEzfKKbt+YUnJCHY4QlJCHZ4QhIit4aP0dbeEtYYV5t3XdRPMgQWdZgX9jlhwgRlV1VVZcfLli1TZZ988omyjx07pmyp3eS8QqE6e6HI1rLP4dDwMa6j2PkbyejRo5WNIb29vb3KRn0sr1VRUaHKmpqalD1ixAhl//nnn8q+dOlSdrxv3z5VhnNB1hxMzP0XKreebTEuWn7hCUkIdnhCEoIdnpCEKMu7txzqUCvlVWx6IamBYpbKFkKeC+tYU1Oj7O3btyu7trZW2Zb/G/Xj0qVLlf35558PeB7PD2+F1mKZF/JbCrzlsdIezJwCntfTyuPGjVP20aNHs+MnnnhClQ0mXdb777+vyjZs2KBsfBfkPFLsda35HDwXxhrkSW/GLzwhCcEOT0hCsMMTkhBF++FjlkjGnBt1HOojtLFeUm/W1dWpsr179yr74Ycfzl1H1EeolRctWqTszz77LDtG3ebNU1i+7LsRO++l4LKWRntrEOS5/vjjjwHPG0IIy5cvV3ZXV5dVbQXW69ChQ8qeOXOmsh955JHs+JlnnlFljz/+uLLPnTunbPmuYKxHTExDofLBwi88IQnBDk9IQpQs440kdomrHIbjENDLmILDcjmMf+edd8zfIr///ruyr1+/nh1jKCYOP6dNm6bsMWPGZMe//vqrKovJABSCHgZ72WeGgpgsu1590L1rhaViqG1nZ6d5bsnFixeV3dDQoGwchmP7vvXWW9nx+vXrVdn+/fuV/eSTTyrbCof1XLDWkL4UGYr5hSckIdjhCUkIdnhCEqLoFFcxeMs/pRvDc8PhDjCvvfaasqV7BTU7uta+/vprZbe2tipbLnnF5ZMvvPCCstva2pTd3d2dHePS2p6eHmXH7Cb7b0hpZRGzu28Iuk3wOTz00EPK9lxYMuS1vb3drCeeC9+z06dPZ8cvvviiKsP5GtT/GBJsEbuUfLDwC09IQrDDE5IQ7PCEJETJQmvzloUQt/OspwHr6+uVLf3fqNlbWlqUvXPnTmVXVlYqW84X/PTTT6qso6ND2Zs2bVJ2dXV1wfOEEMLPP/+sbM8PX8wuoaXEa08rBRcSk85sxowZysaYjOPHjytbpqLCc3nPEOcPJk+ePOBvvSXJ1pzMYPpGKeAXnpCEYIcnJCHY4QlJiJJtNWXtAuql9bF+i6xbt07Z48ePV/YPP/yQHX/xxReqDDU7+l5Rw8s0SRg/jXH3Mr1SCCE0NjZmx5j+ateuXcHCiqVHhmM32ZglnJ4GtWLH0Z/93HPPKRs1vIx1CKF/rIR1XbwnfMZyWSv+9saNG+a55TOIXQ5LPzwhpGSwwxOSEOzwhCREbg3vaSCpWzwdbsUxW1oqhBCam5vNc7/88svZ8cmTJ1UZajz0zf7yyy/KlvHUnjY9fPiwslevXl2wTiH0nw/AtfXWFkp3Yz18jM701s5bKa4wdv6pp55StvecJN776vnprdTbv/32m/m38tredb30YQOdt1j4hSckIdjhCUmIolNcWdlivSytOHSTYav4W1xaiju8nj9/XtkHDx7MjnH4iG4fRKa0CkHfkwzZDaF/2qpvvvlG2fLaDzzwgCobO3assnGoikNEy+U5HG65wez+i21gSRBMf/XBBx+Y9UK3q2wvb0m2tzxWZrXduHGjKsNQadyZR14L79/LujzUYdT8whOSEOzwhCQEOzwhCVG0hreWdHrLB3HZqvxbuaw0hBB2796tbEwvvHbtWvPcEtRaqKdQS1vpl/FvL1y4oGwZBorpljFF0uXLl5Vdyl19SgG2taUzY9J1IRiyihrem0uwrh0b0ipdb1iGc1D4Xsm2j3Fne/UsRdgtv/CEJAQ7PCEJwQ5PSEIUreEtPWGFGobQX2dLDbRt2zZVVl5ermwsl8thQ9C+3NhQREtveToWy2UYL/4W799bTiz/Hsus8NJS4c0bxKQ7s3z4nm/cO1dMLAj6/DHsWoZGe3EHlh/eiqkoVM8YXV7MfA6/8IQkBDs8IQnBDk9IQhSt4S1d7mlSTFUkt4QaN26cKvv222+VjamkLL+up+MQ1FvWVr0YW4B/29vbmx1/+eWXquyrr74y64FYWu1uxNJbKbcQz/9tpbj2/haXTldVVWXHnv5HDS/bK4QQZs2alR3jnMu1a9eUjW0vr+3N/XjzAVZcSTHwC09IQrDDE5IQ7PCEJETJYuljUvOgLde44xbPcvugEPpv+WSlJvLq4flX5e9x3gHvf/HixcquqKjIjjH+H1Nce1o1RjMPBZ6/Vz5zb723tV2097eo2VetWqXszZs3Z8eYvhzvAdvTuha+U7gFlpWGHO/Ji98Y6jkZfuEJSQh2eEISIveQ3nMnyCGTNyzBYczy5cuzYxx2d3V1KdtLmSRtPBe6PBAsx+WyEukCCiGEpqYmZctQzfb2dlXmpd6y7tEbEg4F3rBT1jd2Cae8dymDQghhyZIlyn777beVjWnH5LWx7TAlWUy4t5d26s0331T2xx9/nB1jJmRvZ2CUADE78+aBX3hCEoIdnpCEYIcnJCFKtnusxNIhhVi5cmV2XFNTo8pQ13nLHGVKaNwhxAu3RO0s6/3ggw+qMqnTCtX79ddfz44xlbbnqkJd919yy3l1x3PJHXpbW1tV2Zw5c5SNbjnUxzLsGpdRo2t04sSJyl6xYoWy6+vrs+Pa2lpVhveIKczkjkXPPvusKvvuu++UjW1vpcuKSQ8+EPzCE5IQ7PCEJAQ7PCEJkVvDeymBrVRF3nY6ckksbuMzb948ZX/66afKRv+p/HvU9wj+Ld6TTEe8YcMGVTZlyhRlY6ppGV6JOhbr5elcS6sNR9pqr+1lHfDevJBWaT/22GOqDMOo16xZo+xTp04pW/r0MZU0hjPjtRoaGpR95cqVAa/b1tam7EmTJil76tSpA9YR/xZjNLCeVnwLNTwhxIQdnpCEYIcnJCFKlqZa6jovvht9jdLHjbrkpZdeUvaOHTuU/eOPPw54HZwP8LbAmj59urKPHDlSsI4hhLBnzx5lS797CDplkrcOAYnZbmg4NDy2Z8ySTox9QLuvry87xq3AMY7i5s2byrbmB3Cu4Omnn1b2q6++qmyMy+/s7MyOcYm2LAtBvychhDB79uzsGOc06urqzHPhPctnXYr0V/zCE5IQ7PCEJETuIb2XAVbasTuVvPHGG9nxli1bVNnYsWOVfeLECWVjphm5Ew0OeTBTybp165T9/PPPK1uGTH744Yeq7JVXXlE2XsvatQbBZ4vLZa1lusMxpPd2S7GWj3ouRhk6jRIL2x7DmRcuXKhs6dLCtl6wYMGAvw0hhAMHDij73XffDQOBbd3Y2KhsOWyfP3++KsO29MK/rczJsTsrhcAvPCFJwQ5PSEKwwxOSEGV3copA1JXerpjqIk5WVum6QF3W0dFh1gORugY1obXDayFaWlqy461btw54nULnlveMLj28LrqQ0P1iLY/FZzkUu8mie9MKlcb2wR1dUP9Kd1h3d7cqw7BqBJ+b5RrG54Jhu+gek21vXSeE/q43ee3q6mpVhkt8cem05Xa1UouFkK/t+YUnJCHY4QlJCHZ4QhIit4ZH7YE60/IJop6ydArqoblz5yobQ2sfffRRZUs9iZqnp6dH2c3Nzco+c+aMss+ePZsdx2h2tGXarRD661jUiDg/Yj1bLBsKDY9tb6XkwueA8xXWrrvod3/vvfeUjWnErl69quzKysrsGNOIY70++ugjZWP8x/fff58d47vuzWfJcGF8n7Ee8reFyqnhCSFFww5PSEKwwxOSEEVr+JjdYlHjWMv8vCWsWF3Uk1JflZeXqzLUyqil8VwxKYIt25vD8LbEks/ASi8VQnFLJj08DW/hxXtbW4PFrsmQYJ1RS2NMu1VPy88eQv84BanLvRTk+E4isu29VGN52p5feEISgh2ekIRghyckIXKvh/f87tL2tjRGrSG1GPo4ve2hLBu3CPb0vxUj7q3rtvRkrDZF7naKq5jtv71nbMUzoBZGHz7Ww3o3vLkNfJ+t99tKyV7o3BY4J+VtO2bNl3j6vxD8whOSEOzwhCRE0bvHInJo4rmdrHN5Q0IrnRL+fcwOLrH1jBnSx9bDYjB/O1TXlM/Na3trCBu7q6413EW3HA7/PfevtdTWc0tarkbPtWa9g6Voe37hCUkIdnhCEoIdnpCEyK3hY5Zdxuo46zoxLj2vHrE7wMTopxj3mPdbS8cVk5p4sMS4Ar22x/aU2tnS0SH0b3trGbH3vsa8k14aMWs+x2uvmDBlr1554BeekIRghyckIdjhCUmI3MtjCSH/ffiFJyQh2OEJSQh2eEISgh2ekIRghyckIdjhCUkIdnhCEoIdnpCEYIcnJCH+B7PFERx9X4PaAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
